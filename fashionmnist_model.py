# -*- coding: utf-8 -*-
"""FashionMNIST_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D6Z2BCkvw24_8VabiH2nHuIomJjWtVbm
"""

# cnn model for fashion mnist
import numpy as np
from matplotlib import pyplot
from sklearn.model_selection import KFold
from keras.datasets import fashion_mnist
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D,Dense,MaxPooling2D,Dropout,Flatten, Activation, BatchNormalization,LeakyReLU
from keras.optimizers import SGD
from keras.preprocessing.image import ImageDataGenerator

####Load Dataset####
def load_dataset():
	# load dataset
	(trainX, trainY), (testX, testY) = fashion_mnist.load_data()
	# reshape dataset to have a single channel
	trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))
	testX = testX.reshape((testX.shape[0], 28, 28, 1))
	# one hot encode target values
	trainY = to_categorical(trainY)
	testY = to_categorical(testY)
	return trainX, trainY, testX, testY
trainX, trainY, testX, testY = load_dataset()
print(trainX.shape[0])

####Prepare Pixel Data####
# scale pixels
# convert from integers to floats
trainX = trainX.astype('float32')
testX = testX.astype('float32')
#Centering pixels
trainX -= np.mean(trainX, axis=0)
testX -= np.mean(testX, axis=0)
# # normalize to range 0-1
# trainX = trainX / 255.0
# testX = testX / 255.0
# # return normalized images

# #Data Augmentation
# datagen = ImageDataGenerator()
# datagen.fit(trainX)
# X_batch, y_batch = datagen.flow(trainX, trainX, batch_size=32)
# fit_generator(datagen, samples_per_epoch=len(trainX), epochs=100)
# batch_size = 16
nb_train_samples = trainX.shape[0]
nb_validation_samples = testX.shape[0]
IMG_H = 28
IMG_W = 28
BS = 64
EPOCHS = 10
NUM_TOPICS = 10
DROP_RATE = 0.25
#INIT_LR = 1e-2
# prepare data augmentation configuration
train_datagen = ImageDataGenerator(
        rescale=1./255,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True)

test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow(
        trainX,
        # target_size=(IMG_H, IMG_W),
        batch_size=BS)
        #class_mode='binary')

validation_generator = test_datagen.flow(
        testX,
        # target_size=(IMG_H, IMG_W),
        batch_size=BS)
        # class_mode='binary')

# # fine-tune the model
# model.fit_generator(
#         train_generator,
#         steps_per_epoch=60000 // BS,
#         epochs=EPOCHS,
#         validation_data=validation_generator,
#         validation_steps=10000 // BS)

BS = 64
EPOCHS = 10
NUM_TOPICS = 10
DROP_RATE = 0.25
#INIT_LR = 1e-2

model = Sequential()

model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=(28,28,1),padding='same'))
model.add(BatchNormalization())
model.add(Conv2D(32, (3, 3), activation='relu',padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))
model.add(Dropout(DROP_RATE))

model.add(Conv2D(64, kernel_size=(3, 3),activation='relu',padding='same'))
model.add(BatchNormalization())
model.add(Conv2D(64, (3, 3), activation='relu',padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))
model.add(Dropout(DROP_RATE))

model.add(Conv2D(128, kernel_size=(3, 3),activation='relu',padding='same'))
model.add(BatchNormalization())
model.add(Conv2D(128, (3, 3), activation='relu',padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))
model.add(Dropout(DROP_RATE))

model.add(Conv2D(256, kernel_size=(3, 3),activation='relu',padding='same'))
model.add(BatchNormalization())
model.add(Conv2D(256, (3, 3), activation='relu',padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))
model.add(Dropout(DROP_RATE))

model.add(Flatten())
model.add(Dense(500,use_bias=False,activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(DROP_RATE)) 
model.add(Dense(NUM_TOPICS, activation='softmax'))

#opt = SGD(lr=INIT_LR, momentum=0.9, decay=INIT_LR / EPOCHS)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
history = model.fit(trainX, trainY, epochs=EPOCHS, batch_size=BS, validation_data=(testX, testY),shuffle=True)
acc = model.evaluate(testX, testY, verbose=0)
acc



batch_size = 64
epochs = 10
num_classes = 10
dropout_rate = 0.25
model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=(28,28,1),padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))
model.add(Conv2D(64, (3, 3), activation='relu',padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))
model.add(Dropout(dropout_rate))
model.add(Conv2D(64, (3, 3), activation='relu',padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))
model.add(Dropout(dropout_rate))
model.add(Conv2D(64, (3, 3), activation='relu',padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))
model.add(Dropout(dropout_rate))
model.add(Conv2D(64, (3, 3), activation='relu',padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))
model.add(Flatten())
#model.add(Dense(128, activation='relu'))
model.add(Dense(500,use_bias=False,activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(dropout_rate)) 
#model.add(Flatten())
model.add(Dense(num_classes, activation='softmax'))

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
history = model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, validation_data=(testX, testY),shuffle=True)
acc = model.evaluate(testX, testY, verbose=0)
acc

batch_size = 64
epochs = 10
num_classes = 10
model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=(28,28,1),padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))
model.add(Conv2D(64, (3, 3), activation='relu',padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))
model.add(Dropout(0.25))
model.add(Conv2D(64, (3, 3), activation='relu',padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))
model.add(Dropout(0.25))
model.add(Conv2D(64, (3, 3), activation='relu',padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))
model.add(Dropout(0.25))
model.add(Conv2D(64, (3, 3), activation='relu',padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))
model.add(Flatten())
#model.add(Dense(128, activation='relu'))
model.add(Dense(500,use_bias=False,activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.25)) 
#model.add(Flatten())
model.add(Dense(num_classes, activation='softmax'))

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
history = model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, validation_data=(testX, testY))
acc = model.evaluate(testX, testY, verbose=0)
acc



####Present Results####
# plot diagnostic learning curves
def summarize_diagnostics(histories):
	for i in range(len(histories)):
		# plot loss
		pyplot.subplot(211)
		pyplot.title('Cross Entropy Loss')
		pyplot.plot(histories[i].history['loss'], color='blue', label='train')
		pyplot.plot(histories[i].history['val_loss'], color='orange', label='test')
		# plot accuracy
		pyplot.subplot(212)
		pyplot.title('Classification Accuracy')
		pyplot.plot(histories[i].history['accuracy'], color='blue', label='train')
		pyplot.plot(histories[i].history['val_accuracy'], color='orange', label='test')
	pyplot.show()

# summarize model performance
def summarize_performance(scores):
	# print summary
	print('Accuracy: mean=%.3f std=%.3f, n=%d' % (mean(scores)*100, std(scores)*100, len(scores)))
	# box and whisker plots of results
	pyplot.boxplot(scores)
	pyplot.show()

# run the test harness for evaluating a model
def run_test_harness():
	# load dataset
	trainX, trainY, testX, testY = load_dataset()
	# prepare pixel data
	trainX, testX = prep_pixels(trainX, testX)
	# evaluate model
	scores, histories = evaluate_model(trainX, trainY)
	# learning curves
	summarize_diagnostics(histories)
	# summarize estimated performance
	summarize_performance(scores)

# entry point, run the test harness
run_test_harness()

