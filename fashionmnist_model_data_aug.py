# -*- coding: utf-8 -*-
"""FashionMNIST_model_data_aug.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jSsSz4E71yfKkhmvwFOJxerEHmK-a4NM
"""

# Importing libraries
import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot
from keras.datasets import fashion_mnist
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D,Dense,MaxPooling2D,Dropout,Flatten, Activation, BatchNormalization,LeakyReLU
from keras.optimizers import SGD
from keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report

# Loading Dataset
def load_dataset(img_r,img_c):
	# Loading the dataset
	(trainX_img, trainY_labels), (testX_img, testY_labels) = fashion_mnist.load_data()
	# Reshaping dataset to have a single channel
	trainX = trainX_img.reshape((trainX_img.shape[0], img_r, img_c, 1))
	testX = testX_img.reshape((testX_img.shape[0], img_r, img_c, 1))
	# One hot encode target values
	trainY = to_categorical(trainY_labels)
	testY = to_categorical(testY_labels)
	return trainX, trainY, testX, testY, trainX_img, trainY_labels, testX_img, testY_labels



# Preparing Pixel Data
def prep_pixels(trainX, testX):
  # Scaling the pixels
  # Converting from integers to floats
  trainX = trainX.astype('float32')
  testX = testX.astype('float32')
  # Centering pixels
  #trainX -= np.mean(trainX, axis=0)
  #testX -= np.mean(trainX, axis=0)
  return trainX, testX

# Creating the model architecture
# Building the model & Compile
#def build_model(trainX, trainY,batch_size,epochs, num_topics, drop_rate):
def build_model(trainX, trainY, num_topics, drop_rate):
  model = Sequential()

  model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=(28,28,1),padding='same'))
  model.add(BatchNormalization())
  model.add(Conv2D(32, (3, 3), activation='relu',padding='same'))
  model.add(BatchNormalization())
  model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))
  model.add(Dropout(drop_rate))

  model.add(Conv2D(64, kernel_size=(3, 3),activation='relu',padding='same'))
  model.add(BatchNormalization())
  model.add(Conv2D(64, (3, 3), activation='relu',padding='same'))
  model.add(BatchNormalization())
  model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))
  model.add(Dropout(drop_rate))

  model.add(Conv2D(128, kernel_size=(3, 3),activation='relu',padding='same'))
  model.add(BatchNormalization())
  model.add(Conv2D(128, (3, 3), activation='relu',padding='same'))
  model.add(BatchNormalization())
  model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))
  model.add(Dropout(drop_rate))

  model.add(Conv2D(256, kernel_size=(3, 3),activation='relu',padding='same'))
  model.add(BatchNormalization())
  model.add(Conv2D(256, (3, 3), activation='relu',padding='same'))
  model.add(BatchNormalization())
  model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))
  model.add(Dropout(drop_rate))

  model.add(Flatten())
  model.add(Dense(500,activation='relu'))
  model.add(BatchNormalization())
  model.add(Dropout(drop_rate)) 
  model.add(Dense(num_topics, activation='softmax'))
  # Compile the model
  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
  return model



#  Image Data Augumentation & Training
def fit_model(model, trainX, trainY, testX, testY, batch_size,epochs):
  gen = ImageDataGenerator(
          rescale=1./255,
          rotation_range=20,
          shear_range=0.2,
          zoom_range=0.2,
          horizontal_flip=True)
  test_gen = ImageDataGenerator(rescale=1./255)

  batches = gen.flow(trainX, trainY, batch_size=batch_size)
  test_batches = test_gen.flow(testX, testY, batch_size=batch_size)#, shuffle=False)

  # Train the model
  history = model.fit_generator(batches, steps_per_epoch=trainX.shape[0]//batch_size, epochs=epochs,validation_data=test_batches,
                                                    validation_steps=testX.shape[0]//batch_size)
  score = model.evaluate_generator(test_batches,steps=testX.shape[0]//batch_size, verbose=0)
  return score ,history, model, test_batches

# Plotting Accuracy & Loss Curves
def curves(model, epochs):
  acc = model.history['accuracy']
  val_acc = model.history['val_accuracy']
  loss = model.history['loss']
  val_loss = model.history['val_loss']
  # Plots
  plt.plot(range(epochs), acc, 'mo', label='Training accuracy')
  plt.plot(range(epochs), val_acc, 'b', label='Validation accuracy')
  plt.title('Training and validation accuracy')
  plt.legend()
  plt.figure()
  plt.plot(range(epochs), loss, 'mo', label='Training loss')
  plt.plot(range(epochs), val_loss, 'b', label='Validation loss')
  plt.title('Training and validation loss')
  plt.legend()
  plt.show() 


#Confusion Matrix and Classification Report
#def confusion_mat(model,Y_pred, testY, testY_labels, class_labels):
def confusion_mat(model,Y_pred, testY_labels, class_labels):
  y_pred = np.argmax(Y_pred, axis=1)
  confusion_mat = confusion_matrix(testY_labels, y_pred)
  classification_rep = classification_report(testY_labels, y_pred, target_names=class_labels)
  plt.figure(figsize=(10,7))
  df_confusion_mat = pd.DataFrame(confusion_mat)
  sns.heatmap(df_confusion_mat, annot_kws={"size": 10}, linewidths=.5, cmap='PuBu', annot=True,
            yticklabels=class_labels, xticklabels=class_labels, fmt='g')
  plt.xticks(rotation=40) 
  plt.ylabel('True label')
  plt.xlabel('Predicted label')
  confusion_mat_plt = plt
  return  confusion_mat_plt, classification_rep



# Plotting an Image with True Label & Predicted Label
def plot_img(i, predictions_array, true_label, img, class_labels):
  predictions_array, true_label, img = predictions_array, true_label[i], img[i]
  plt.grid(False)
  plt.xticks([])
  plt.yticks([])
  plt.imshow(img, cmap=plt.cm.binary)
  predicted_label = np.argmax(predictions_array)
  if predicted_label == true_label:
    color = 'lightseagreen'
  else:
    color = 'crimson'
  plt.xlabel("{} {:2.0f}% ({})".format(class_labels[predicted_label],
                                100*np.max(predictions_array),
                                class_labels[true_label]),
  
  

#Plotting Number of Images with True Label & Predicted Label (using plot_image function)
def predicted_imgs(Y_pred, testY_labels, testX_img, class_labels, plt_num_rows, plt_num_cols):
  num_images = plt_num_rows*plt_num_cols
  plt.figure(figsize=(2*2*plt_num_cols, 2*plt_num_rows))
  for i in range(num_images):
    plt.subplot(plt_num_rows, 2*plt_num_cols, 2*i+1)
    plot_img(i, Y_pred[i], testY_labels, testX_img, class_labels)
  plt.show()
                              color=color,fontsize=10)

# Setting the Parameters & Hyperparameters
BS = 256
EPOCHS = 25
NUM_TOPICS = 10
DROP_RATE = 0.25
IMG_R, IMG_C = 28, 28
CLASSES_LABELS = ['T-shirt/top', 'Trouser/pants', 'Pullover shirt', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
PLT_NUM_ROWS= 10
PLT_NUM_COLS = 3

# loading the dataset
trainX, trainY, testX, testY,trainX_img, trainY_labels, testX_img, testY_labels  = load_dataset(IMG_R, IMG_C)

# preparing pixel data
trainX, testX = prep_pixels(trainX, testX)

#Building the model
model = build_model(trainX, trainY, NUM_TOPICS, DROP_RATE)

# Training & evaluating the model
scores, histories, model, test_batches = fit_model(model, trainX, trainY, testX, testY, BS, EPOCHS)

# Learning curves
curves(histories, EPOCHS)

print('Accuracy:{} \nLoss:{}'.format(scores[1] ,scores[0]))

# Prediction
Y_pred = model.predict_generator(test_batches,steps=testY.shape[0]//BS+1)

#test_batches.reset()
confusion_mat, classification_rep = confusion_mat(model,Y_pred, testY, testY_labels, BS, CLASSES_LABELS)

print(classification_rep)

# Ploting test images with their predicted labels and the true labels.
predicted_imgs(Y_pred, testY_labels, testX_img, CLASSES_LABELS, PLT_NUM_ROWS, PLT_NUM_COLS)